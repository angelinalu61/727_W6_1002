webpage <- read_html(url)
tables <- webpage %>% html_table(fill = TRUE)
new_pop <- tables[[2]]
new_pop <- new_pop[2:10, -3]
pops <- cbind(pops, new_pop$Pop.)
}
for(i in places_east) {
url <- paste0("https://en.wikipedia.org/wiki/", i)
webpage <- read_html(url)
tables <- webpage %>% html_table(fill = TRUE)
new_pop <- tables[[2]]
new_pop <- new_pop[2:10, -3]
pops <- cbind(pops, new_pop$Pop.)
}
for(i in places_east) {
url <- paste0("https://en.wikipedia.org/wiki/Washington_Park,_Chicago", i)
webpage <- read_html(url)
tables <- webpage %>% html_table(fill = TRUE)
new_pop <- tables[[2]]
new_pop <- new_pop[2:10, -3]
pops <- cbind(pops, new_pop$Pop.)
}
library(rvest)
library(tibble)
# 創建一個空的 tibble 來存儲地區名稱和描述文本
descriptions <- tibble(location = character(), description = character())
# 從每個地區的 Wikipedia 頁面提取文本描述
for (i in places_east) {
url <- paste0("https://en.wikipedia.org/wiki/", i)
webpage <- tryCatch(read_html(url), error = function(e) NA)
if (!is.na(webpage)) {
# 提取主要文本內容，選擇 HTML 標籤來獲取段落文本
text <- webpage %>% html_nodes("p") %>% html_text()
# 將文本合併成一段
description <- paste(text, collapse = " ")
# 將地區名稱和描述文本添加到 tibble 中
descriptions <- add_row(descriptions, location = i, description = description)
}
}
# 查看抓取的描述文本
print(descriptions)
pops <- pop
for(i in places_east) {
url <- paste0("https://en.wikipedia.org/wiki/", i)
}
print(url)
for(i in places_east) {
url <- paste0("https://en.wikipedia.org/wiki/", i)
webpage <- read_html(url)
tables <- webpage %>% html_table(fill = TRUE)
new_pop <- tables[[2]]
new_pop <- new_pop[2:10, -3]
pops <- cbind(pops, new_pop$Pop.)
}
library(rvest)
library(tibble)
descriptions <- tibble(location = character(), description = character())
for (i in places_east) {
url <- paste0("https://en.wikipedia.org/wiki/", i)
webpage <- tryCatch(read_html(url), error = function(e) NA)
if (!is.na(webpage)) {
text <- webpage %>% html_nodes("p") %>% html_text()
description <- paste(text, collapse = " ")
descriptions <- add_row(descriptions, location = i, description = description)
}
}
print(descriptions)
descriptions <- tibble(location = character(), description = character())
for (i in places_east) {
url <- paste0("https://en.wikipedia.org/wiki/", i)
webpage <- tryCatch(read_html(url), error = function(e) NA)
if (!is.na(webpage)) {
text <- webpage %>% html_nodes("p") %>% html_text()
description <- paste(text, collapse = " ")
descriptions <- add_row(descriptions, location = i, description = description)
}
}
print(descriptions)
descriptions <- tibble(location = character(), description = character())
for (i in places_east) {
url <- paste0("https://en.wikipedia.org/wiki/", i)
webpage <- tryCatch(read_html(url), error = function(e) NA)
if (!is.na(webpage)) {
text <- webpage %>% html_nodes("p") %>% html_text()
description <- paste(text, collapse = " ")
descriptions <- add_row(descriptions, location = i, description = description)
}
}
print(descriptions)
# 計算最常見的詞語
common_words <- tidy_descriptions %>%
count(word, sort = TRUE)
descriptions <- tibble(location = character(), description = character())
for (i in places_east) {
url <- paste0("https://en.wikipedia.org/wiki/", i)
webpage <- tryCatch(read_html(url), error = function(e) NA)
if (!is.na(webpage)) {
text <- webpage %>% html_nodes("p") %>% html_text()
description <- paste(text, collapse = " ")
descriptions <- add_row(descriptions, location = i, description = description)
}
}
print(descriptions)
# 將描述文本轉換為一行一詞格式
tidy_descriptions <- descriptions %>%
unnest_tokens(word, description)
options(repos = c(CRAN = "http://cran.r-project.org"))
knitr::opts_chunk$set(echo = TRUE,cache=TRUE,
autodep=TRUE, cache.comments=FALSE,
message=FALSE, warning=FALSE,
fig.width=4.5, fig.height=3)
library(xml2)
library(rvest)
library(tidyverse)
library(jsonlite)
library(robotstxt)
library(RSocrata)
options(repos = c(CRAN = "http://cran.r-project.org"))
knitr::opts_chunk$set(echo = TRUE,cache=TRUE,
autodep=TRUE, cache.comments=FALSE,
message=FALSE, warning=FALSE,
fig.width=4.5, fig.height=3)
library(xml2)
library(rvest)
library(tidyverse)
library(jsonlite)
library(robotstxt)
library(RSocrata)
library(tidytext)
library(dplyr)
paths_allowed("https://en.wikipedia.org/wiki/Grand_Boulevard,_Chicago")
url <- read_html("https://en.wikipedia.org/wiki/Grand_Boulevard,_Chicago")
tables <- url %>% html_table(fill = TRUE)
str(tables)
pop <- pop[2:10, -3]
adjacent_places <- tables[[3]]
print(adjacent_places)
places_east <- adjacent_places[[2]][!is.na(adjacent_places[[2]])]
places_east <- gsub(" ", "_", places_east)
print(places_east)
pops <- pop
for(i in places_east) {
url <- paste0("https://en.wikipedia.org/wiki/", i)
}
print(url)
for(i in places_east) {
url <- paste0("https://en.wikipedia.org/wiki/", i)
webpage <- read_html(url)
tables <- webpage %>% html_table(fill = TRUE)
new_pop <- tables[[2]]
new_pop <- new_pop[2:10, -3]
pops <- cbind(pops, new_pop$Pop.)
}
descriptions <- tibble(location = character(), description = character())
for (i in places_east) {
url <- paste0("https://en.wikipedia.org/wiki/", i)
webpage <- tryCatch(read_html(url), error = function(e) NA)
if (!is.na(webpage)) {
text <- webpage %>% html_nodes("p") %>% html_text()
description <- paste(text, collapse = " ")
descriptions <- add_row(descriptions, location = i, description = description)
}
}
print(descriptions)
# 將描述文本轉換為一行一詞格式
tidy_descriptions <- descriptions %>%
unnest_tokens(word, description)
# 移除停用詞（例如 "the", "and", "in" 等常見詞語）
data("stop_words")
tidy_descriptions <- tidy_descriptions %>%
anti_join(stop_words, by = "word")
# 查看清理後的文本
print(tidy_descriptions)
options(repos = c(CRAN = "http://cran.r-project.org"))
knitr::opts_chunk$set(echo = TRUE,cache=TRUE,
autodep=TRUE, cache.comments=FALSE,
message=FALSE, warning=FALSE,
fig.width=4.5, fig.height=3)
library(xml2)
library(rvest)
library(tidyverse)
library(tidytext)
pop <- tables[[3]]
pop <- pop[2:10, -3]
print(pop)
adjacent_places <- tables[[4]]
places_east <- adjacent_places$East %>% na.omit() %>% as.character()
print(places_east)
places_east <- gsub(" ", "_", places_east)
# The updated vector should now look like: "Oakland,_Chicago" "Kenwood,_Chicago" "Hyde_Park,_Chicago"
print(places_east)
adjacent_places <- tables[[3]]
print(adjacent_places)
places_east <- adjacent_places[[2]][!is.na(adjacent_places[[2]])]
places_east <- gsub(" ", "_", places_east)
print(places_east)
places_east <- adjacent_places[[2]][!is.na(adjacent_places[[2]])]
places_east <- gsub(",$", "", places_east)
print(places_east)
places_east <- adjacent_places[[2]][!is.na(adjacent_places[[2]])]
places_east <- gsub(",", "", places_east)
print(places_east)
places_east <- adjacent_places[[2]][!is.na(adjacent_places[[2]])]
# Replace spaces with underscores (if not already done)
places_east <- gsub(" ", "_", places_east)
# Remove the comma from the strings while keeping the underscores
places_east <- gsub(",", "", places_east)
# Print the result
print(places_east)
places_east <- adjacent_places[[2]][!is.na(adjacent_places[[2]])]
places_east <- gsub(" ", "_", places_east)
places_east <- gsub(",", "", places_east)
print(places_east)
places_east <- adjacent_places[[2]][!is.na(adjacent_places[[2]])]
places_east <- gsub(" ", "_", places_east)
print(places_east)
for (i in places_east) {
url <- paste0("https://en.wikipedia.org/wiki/", i)
}
print(url)
# Initialize a copy of the pop table
pops <- pop
# Loop through each community area to gather population tables
for (i in places_east) {
url <- paste0("https://en.wikipedia.org/wiki/", i)
page <- read_html(url)
tables <- html_table(page, fill = TRUE)
new_pop <- tables[[3]]  # Adjust the index if necessary
new_pop <- new_pop[2:10, -3]  # Clean the table
pops <- cbind(pops, new_pop)
}
# Initialize a copy of the original pop table to store all the data
pops <- pop
# Loop through each community area to gather population tables
for (i in places_east) {
# Create the URL for the current community area
url <- paste0("https://en.wikipedia.org/wiki/", i)
# Read the HTML page of the community area
page <- read_html(url)
# Extract tables from the page
tables <- html_table(page, fill = TRUE)
# Check if the table is available and extract the "Historical population" table
if (length(tables) >= 3) {  # Adjust index if necessary based on table position
new_pop <- tables[[3]]  # Assuming the "Historical population" table is the 3rd table
new_pop <- new_pop[2:10, -3]  # Clean the table by removing unwanted rows/columns
# Append the new population data to the original pops table using cbind()
pops <- cbind(pops, new_pop)
}
}
# Initialize a copy of the original pop table to store all the data
pops <- pop
# Loop through each community area to gather population tables
for (i in places_east) {
# Create the URL for the current community area
url <- paste0("https://en.wikipedia.org/wiki/", i)
# Print the URL to check if it's correctly formatted
print(url)
# Read the HTML page of the community area
page <- tryCatch({
read_html(url)
}, error = function(e) {
message(paste("Error reading URL:", url))
return(NULL)
})
# Proceed only if the page was successfully read
if (!is.null(page)) {
# Extract tables from the page
tables <- html_table(page, fill = TRUE)
# Check if the table is available and extract the "Historical population" table
if (length(tables) >= 3) {  # Adjust index if necessary based on table position
new_pop <- tables[[3]]  # Assuming the "Historical population" table is the 3rd table
new_pop <- new_pop[2:10, -3]  # Clean the table by removing unwanted rows/columns
# Append the new population data to the original pops table using cbind()
pops <- cbind(pops, new_pop)
} else {
message(paste("Historical population table not found for:", i))
}
}
}
# Display the combined table with all population data
print(pops)
pops <- pop
for (i in places_east) {
url <- paste0("https://en.wikipedia.org/wiki/", i)
print(url)
page <- tryCatch({
read_html(url)
}, error = function(e) {
message(paste("Error reading URL:", url))
return(NULL)
})
if (!is.null(page)) {
tables <- html_table(page, fill = TRUE)
if (length(tables) >= 3) {
new_pop <- tables[[3]]
new_pop <- new_pop[2:10, -3]
pops <- cbind(pops, new_pop)
} else {
message(paste("Historical population table not found for:", i))
}
}
}
print(pops)
# Function to extract the main text description from a page
get_description <- function(url) {
page <- read_html(url)
description <- page %>%
html_nodes("p") %>%  # Extract paragraphs
html_text() %>%
paste(collapse = ' ')  # Combine text into a single block
return(description)
}
# Create a tibble with location names and descriptions
descriptions <- tibble(
location = places_east,
text = sapply(paste0("https://en.wikipedia.org/wiki/", places_east), get_description)
)
# Function to extract the main text description from a page
get_description <- function(url) {
page <- tryCatch({
read_html(url)
}, error = function(e) {
message(paste("Error reading URL:", url))
return(NULL)
})
if (!is.null(page)) {
description <- page %>%
html_nodes("p") %>%  # Extract paragraphs
html_text() %>%
paste(collapse = ' ')  # Combine text into a single block
return(description)
} else {
return(NA)  # Return NA if the page couldn't be read
}
}
# Ensure that places_east is correctly formatted
places_east <- gsub(" ", "_", places_east)  # Replace spaces with underscores
places_east <- gsub(",", "", places_east)  # Remove commas from the names
# Create a tibble with location names and descriptions
descriptions <- tibble(
location = places_east,
text = sapply(paste0("https://en.wikipedia.org/wiki/", places_east), get_description)
)
# Display the tibble with the location names and their descriptions
print(descriptions)
get_description <- function(url) {
page <- tryCatch({
read_html(url)
}, error = function(e) {
message(paste("Error reading URL:", url))
return(NULL)
})
if (!is.null(page)) {
description <- page %>%
html_nodes("p") %>%
html_text() %>%
paste(collapse = ' ')
return(description)
} else {
return(NA)
}
}
places_east <- gsub(" ", "_", places_east)
places_east <- gsub(",", "", places_east)
descriptions <- tibble(
location = places_east,
text = sapply(paste0("https://en.wikipedia.org/wiki/", places_east), get_description)
)
print(descriptions)
tidy_data <- descriptions %>%
unnest_tokens(word, text) %>%  # Tokenize the text into words
anti_join(stop_words)  # Remove stop words
# Find the most common words used overall
common_words <- tidy_data %>%
count(word, sort = TRUE)
print(common_words)
# Plot the most common words within each location
tidy_data %>%
count(location, word, sort = TRUE) %>%
group_by(location) %>%
top_n(10) %>%
ggplot(aes(reorder(word, n), n, fill = location)) +
geom_col(show.legend = FALSE) +
facet_wrap(~ location, scales = "free_y") +
coord_flip() +
labs(title = "Most Common Words in Each Location", x = "Word", y = "Frequency")
# Load the tidytext package
library(tidytext)
# Clean the data by converting it into tokens
tidy_data <- descriptions %>%
unnest_tokens(word, text) %>%  # Tokenize the text into individual words
anti_join(stop_words)  # Remove stop words
# Display the first few rows of the tidy_data to verify
print(head(tidy_data))
# Load the tidytext package
library(tidytext)
# Clean the data by converting it into tokens
tidy_data <- descriptions %>%
unnest_tokens(word, text) %>%  # Tokenize the text into individual words
anti_join(stop_words)  # Remove stop words
# Display the first few rows of the tidy_data to verify
print(head(tidy_data))
common_words <- tidy_data %>%
count(word, sort = TRUE)  # Count the occurrences of each word
# Display the top 10 most common words
print(head(common_words, 10))
# Load the tidytext package
library(tidytext)
# Clean the data by converting it into tokens
tidy_data <- descriptions %>%
unnest_tokens(word, text) %>%  # Tokenize the text into individual words
anti_join(stop_words)  # Remove stop words
# Display the first few rows of the tidy_data to verify
print(head(tidy_data))
common_words <- tidy_data %>%
count(word, sort = TRUE)  # Count the occurrences of each word
# Display the top 10 most common words
print(head(common_words, 10))
tidy_data %>%
count(location, word, sort = TRUE) %>%
group_by(location) %>%
top_n(10, n) %>%  # Select the top 10 words for each location
ungroup() %>%
ggplot(aes(reorder(word, n), n, fill = location)) +
geom_col(show.legend = FALSE) +
facet_wrap(~ location, scales = "free_y") +
coord_flip() +
labs(title = "Most Common Words in Each Location",
x = "Word",
y = "Frequency",
caption = "Top 10 words used in descriptions of each community area")
# Load the tidytext package
library(tidytext)
# Clean the data by converting it into tokens
tidy_data <- descriptions %>%
unnest_tokens(word, text) %>%  # Tokenize the text into individual words
anti_join(stop_words)  # Remove stop words
# Display the first few rows of the tidy_data to verify
print(head(tidy_data))
common_words <- tidy_data %>%
count(word, sort = TRUE)  # Count the occurrences of each word
# Display the top 10 most common words
print(head(common_words, 10))
library(ggplot2)
# Remove rows with NA or blank words
tidy_data <- tidy_data %>%
filter(!is.na(word), word != "")
# Plot the most common words within each location with better formatting
tidy_data %>%
count(location, word, sort = TRUE) %>%
group_by(location) %>%
top_n(10, n) %>%  # Select the top 10 words for each location
ungroup() %>%
ggplot(aes(x = reorder(word, n), y = n, fill = location)) +
geom_col(show.legend = FALSE) +
facet_wrap(~ location, scales = "free_y") +
coord_flip() +
labs(title = "Most Common Words in Each Location",
x = "Word",
y = "Frequency",
caption = "Top 10 words used in descriptions of each community area") +
theme_minimal() +  # Use a cleaner theme for better readability
theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels
tidy_data <- descriptions %>%
unnest_tokens(word, text) %>%
anti_join(stop_words)
print(head(tidy_data))
common_words <- tidy_data %>%
count(word, sort = TRUE)
print(head(common_words, 10))
library(ggplot2)
# Remove rows with NA or blank words
tidy_data <- tidy_data %>%
filter(!is.na(word), word != "")
# Plot the most common words within each location with better formatting
tidy_data %>%
count(location, word, sort = TRUE) %>%
group_by(location) %>%
top_n(10, n) %>%  # Select the top 10 words for each location
ungroup() %>%
ggplot(aes(x = reorder(word, n), y = n, fill = location)) +
geom_col(show.legend = FALSE) +
facet_wrap(~ location, scales = "free_y") +
coord_flip() +
labs(title = "Most Common Words in Each Location",
x = "Word",
y = "Frequency",
caption = "Top 10 words used in descriptions of each community area") +
theme_minimal() +  # Use a cleaner theme for better readability
theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels
tidy_data <- descriptions %>%
unnest_tokens(word, text) %>%
anti_join(stop_words)
print(head(tidy_data))
common_words <- tidy_data %>%
count(word, sort = TRUE)
print(head(common_words, 10))
library(ggplot2)
tidy_data <- tidy_data %>%
filter(!is.na(word), word != "")
tidy_data %>%
count(location, word, sort = TRUE) %>%
group_by(location) %>%
top_n(10, n) %>%
ungroup() %>%
ggplot(aes(x = reorder(word, n), y = n, fill = location)) +
geom_col(show.legend = FALSE) +
facet_wrap(~ location, scales = "free_y") +
coord_flip() +
labs(x = "Word",
y = "Frequency") +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
